{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQasANiZADcM",
    "outputId": "f1d8c640-a2aa-426a-d46b-cbf6ec195391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
      "\u001B[?25hCollecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001B[K     |████████████████████████████████| 198 kB 39.9 MB/s \n",
      "\u001B[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=f86a67de62b3a0ebee8524059651d73ac2ed5540b1c06a10f42952e981b7ace5\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrhVkwkS-rUJ"
   },
   "source": [
    "SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dBqlaHr-ssS"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = 'DataEngineering'\n",
    "\n",
    "spark = SparkSession \\\n",
    "      .builder \\\n",
    "      .enableHiveSupport() \\\n",
    "      .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "      .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "      .appName(app_name) \\\n",
    "      .getOrCreate()\\\n",
    "\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHUMN_GDuSNT"
   },
   "source": [
    "Operations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVG72oF2w39K"
   },
   "source": [
    "*Fake Friends*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "fakefriends.csv schema:\n",
    "\n",
    "      id\n",
    "      name\n",
    "      age\n",
    "      number_of_friends (on social media)\n",
    "\n",
    "\n",
    "დავთვალოთ თითოეული ასაკისთვის სოციალურ ქსელში მეგობრების საშუალო რაოდენობა RDD-ების და Map/MapValues ფუნქციის გამოყენებით"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef7h5nQ4p6Yi",
    "outputId": "f4d7d245-9cb6-4405-f1c3-22826d00bedf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 242.05882352941177)\n",
      "(40, 250.8235294117647)\n",
      "(68, 269.6)\n",
      "(54, 278.0769230769231)\n",
      "(38, 193.53333333333333)\n",
      "(56, 306.6666666666667)\n",
      "(36, 246.6)\n",
      "(22, 206.42857142857142)\n",
      "(60, 202.71428571428572)\n",
      "(30, 235.8181818181818)\n",
      "(42, 303.5)\n",
      "(48, 281.4)\n",
      "(50, 254.6)\n",
      "(32, 207.9090909090909)\n",
      "(58, 116.54545454545455)\n",
      "(64, 281.3333333333333)\n",
      "(52, 340.6363636363636)\n",
      "(24, 233.8)\n",
      "(20, 165.0)\n",
      "(62, 220.76923076923077)\n",
      "(44, 282.1666666666667)\n",
      "(28, 209.1)\n",
      "(66, 276.44444444444446)\n",
      "(46, 223.69230769230768)\n",
      "(18, 343.375)\n",
      "(34, 245.5)\n",
      "(33, 325.3333333333333)\n",
      "(55, 295.53846153846155)\n",
      "(59, 220.0)\n",
      "(37, 249.33333333333334)\n",
      "(27, 228.125)\n",
      "(53, 222.85714285714286)\n",
      "(57, 258.8333333333333)\n",
      "(43, 230.57142857142858)\n",
      "(35, 211.625)\n",
      "(45, 309.53846153846155)\n",
      "(67, 214.625)\n",
      "(19, 213.27272727272728)\n",
      "(51, 302.14285714285717)\n",
      "(25, 197.45454545454547)\n",
      "(21, 350.875)\n",
      "(49, 184.66666666666666)\n",
      "(39, 169.28571428571428)\n",
      "(31, 267.25)\n",
      "(41, 268.55555555555554)\n",
      "(69, 235.2)\n",
      "(65, 298.2)\n",
      "(61, 256.22222222222223)\n",
      "(29, 215.91666666666666)\n",
      "(47, 233.22222222222223)\n",
      "(63, 384.0)\n",
      "(23, 246.3)\n"
     ]
    }
   ],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "lines = sc.textFile('fakefriends.csv')\n",
    "\n",
    "rdd = lines.map(parseLine)\n",
    "\n",
    "totals_by_age = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "totals_by_age.collect()\n",
    "\n",
    "avg_by_age = totals_by_age.mapValues(lambda x: x[0] / x[1])\n",
    "# avg_by_age = totals_by_age.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "results = avg_by_age.collect()\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27Klr-zFyGWl"
   },
   "source": [
    "*Temperatures in 1800*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "temperatures.csv schema:\n",
    "\n",
    "      station_id\n",
    "      date\n",
    "      entry_type\n",
    "      degrees (C)\n",
    "\n",
    "\n",
    "დავთვალოთ თითოეული მეტეოროლოგიური სადგურისთვის დაფიქსირებული უდაბლესი ტემპერატურა (entry_type = 'TMIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44UJWv0gyJ7i",
    "outputId": "8923768c-a562-4506-a663-62814c462709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t-14.80C\n",
      "EZE00100082\t-13.50C\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    station_id = fields[0]\n",
    "    entry_type = fields[2]\n",
    "    temperature = float(fields[3])\n",
    "    return (station_id, entry_type, temperature)\n",
    "\n",
    "lines = sc.textFile('temperatures.csv')\n",
    "parsedLines = lines.map(parseLine)\n",
    "\n",
    "minTemps = parsedLines.filter(lambda x: 'TMIN' in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + '\\t{:.2f}C'.format(result[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AxMEznB3YqE"
   },
   "source": [
    "*Word Count*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "book.txt ფაილში დავთვალოთ თითოეული სიტყვა რამდენჯერ გვხვდება."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYshG-_83d7Q"
   },
   "outputs": [],
   "source": [
    "'''#1 CountByValue'''\n",
    "\n",
    "import re\n",
    "\n",
    "book = sc.textFile(\"book.txt\")\n",
    "words = book.flatMap(lambda x: x.split())\n",
    "word_counts = words.countByValue()\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "        print(word + \" \" + str(count))\n",
    "\n",
    "''' convert dict to rdd '''\n",
    "dict_to_rdd = sc.parallelize(list(word_counts.items()))\n",
    "# dict_to_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EM4DEIzY37Ck",
    "outputId": "a91db56b-dfd2-4776-c090-8f850f7d5f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to   1789\n",
      "your   1339\n",
      "you   1267\n",
      "the   1176\n",
      "a   1148\n",
      "of   941\n",
      "and   901\n",
      "that   641\n",
      "in   552\n",
      "is   531\n"
     ]
    }
   ],
   "source": [
    "# თან დავსორტოთ კლებადობით და ტოპ 10 ვნახოთ\n",
    "'''#2 Map/ReduceByKey'''\n",
    "\n",
    "import re\n",
    "\n",
    "book = sc.textFile(\"book.txt\")\n",
    "words = book.flatMap(lambda x: x.split())\n",
    "\n",
    "word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "sorted = word_counts.map(lambda x: (x[1], x[0])).sortByKey(ascending=False)\n",
    "results = sorted.collect()[:10]\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1]\n",
    "    print(word + \"   \" + count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5P0NLNvDDHi"
   },
   "source": [
    "Operations on DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEkWp4jRDQcV"
   },
   "source": [
    "*Fake Friends*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "fakefriends.csv schema:\n",
    "\n",
    "      id\n",
    "      name\n",
    "      age\n",
    "      number_of_friends (on social media)\n",
    "\n",
    "\n",
    "დავთვალოთ თითოეული ასაკისთვის სოციალურ ქსელში მეგობრების საშუალო რაოდენობა Spark-ის დატაფრეიმების გამოყენებით"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6JZisI8DT1a",
    "outputId": "43e00242-0dad-4d14-cf17-24eb2d2a4b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+\n",
      "|age|avg(number_of_friends)|\n",
      "+---+----------------------+\n",
      "| 31|                267.25|\n",
      "| 65|                 298.2|\n",
      "| 53|    222.85714285714286|\n",
      "| 34|                 245.5|\n",
      "| 28|                 209.1|\n",
      "| 26|    242.05882352941177|\n",
      "| 27|               228.125|\n",
      "| 44|     282.1666666666667|\n",
      "| 22|    206.42857142857142|\n",
      "| 47|    233.22222222222223|\n",
      "| 52|     340.6363636363636|\n",
      "| 40|     250.8235294117647|\n",
      "| 20|                 165.0|\n",
      "| 57|     258.8333333333333|\n",
      "| 54|     278.0769230769231|\n",
      "| 48|                 281.4|\n",
      "| 19|    213.27272727272728|\n",
      "| 64|     281.3333333333333|\n",
      "| 41|    268.55555555555554|\n",
      "| 43|    230.57142857142858|\n",
      "+---+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "ან მეორენაირად: \n",
      "+---+------------------+\n",
      "|age|avg_num_of_friends|\n",
      "+---+------------------+\n",
      "| 18|            343.38|\n",
      "| 19|            213.27|\n",
      "| 20|            165.00|\n",
      "| 21|            350.88|\n",
      "| 22|            206.43|\n",
      "| 23|            246.30|\n",
      "| 24|            233.80|\n",
      "| 25|            197.45|\n",
      "| 26|            242.06|\n",
      "| 27|            228.13|\n",
      "| 28|            209.10|\n",
      "| 29|            215.92|\n",
      "| 30|            235.82|\n",
      "| 31|            267.25|\n",
      "| 32|            207.91|\n",
      "| 33|            325.33|\n",
      "| 34|            245.50|\n",
      "| 35|            211.63|\n",
      "| 36|            246.60|\n",
      "| 37|            249.33|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "friends = spark\\\n",
    "        .read\\\n",
    "        .option('header', 'false')\\\n",
    "        .option('inferSchema', 'true')\\\n",
    "        .csv('fakefriends.csv')\\\n",
    "        .toDF('id', 'name', 'age', 'number_of_friends')\n",
    "\n",
    "friends\\\n",
    "  .groupBy('age')\\\n",
    "  .mean('number_of_friends')\\\n",
    "  .show()\n",
    "\n",
    "print('ან მეორენაირად: ')\n",
    "\n",
    "friends\\\n",
    "  .groupBy('age')\\\n",
    "  .agg(f.avg('number_of_friends').cast('decimal(10, 2)').alias('avg_num_of_friends'))\\\n",
    "  .orderBy('age', ascending=True) \\\n",
    "  .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5km793QERAT",
    "outputId": "557b516e-9711-46c8-947f-a93406af0b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------\n",
      " name              | Will     \n",
      " age               | 33       \n",
      " number_of_friends | 385      \n",
      "-RECORD 1---------------------\n",
      " name              | Jean-Luc \n",
      " age               | 26       \n",
      " number_of_friends | 2        \n",
      "-RECORD 2---------------------\n",
      " name              | Hugh     \n",
      " age               | 55       \n",
      " number_of_friends | 221      \n",
      "-RECORD 3---------------------\n",
      " name              | Deanna   \n",
      " age               | 40       \n",
      " number_of_friends | 465      \n",
      "-RECORD 4---------------------\n",
      " name              | Quark    \n",
      " age               | 68       \n",
      " number_of_friends | 21       \n",
      "-RECORD 5---------------------\n",
      " name              | Weyoun   \n",
      " age               | 59       \n",
      " number_of_friends | 318      \n",
      "-RECORD 6---------------------\n",
      " name              | Gowron   \n",
      " age               | 37       \n",
      " number_of_friends | 220      \n",
      "-RECORD 7---------------------\n",
      " name              | Will     \n",
      " age               | 54       \n",
      " number_of_friends | 307      \n",
      "-RECORD 8---------------------\n",
      " name              | Jadzia   \n",
      " age               | 38       \n",
      " number_of_friends | 380      \n",
      "-RECORD 9---------------------\n",
      " name              | Hugh     \n",
      " age               | 27       \n",
      " number_of_friends | 181      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends\\\n",
    "  .select('name', 'age', 'number_of_friends')\\\n",
    "  .limit(10)\\\n",
    "  .show(truncate=0, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VKBwXzIS5MZ"
   },
   "source": [
    "*Breaking News*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "დატასეტში bbc_news.csv მოცემული გვაქვს 1500-მდე მოკლე სტატია BBC News-იდან. თითოეულ სტატიას აქვს უნიკალური ID და კატეგორიის label-ი (tech, business, politics, entertainment, sport).\n",
    "\n",
    "მიზანია, რომ თითოეულ სტატიას დავუგენერიროთ tag-ები შემდეგი მარტივი კრიტერიუმით:  უნდა მოვძებნოთ 3 ყველაზე ხშირი სიტყვა ამ სტატიაში. აუცილებელია, რომ თითოეული სიტყვა მინიმუმ 10-ჯერ მაინც გვხვდებოდეს კონკრეტულ სტატიაში.\n",
    "\n",
    "გაითვალისწინეთ, რომ:\n",
    " * Tag-ები არ უნდა იყოს ე.წ. stopword-ები ან სიმბოლოები. მათი ჩამონათვალი არის ფაილში stopwords.txt;\n",
    " * Tag-ები არ შეიძლება იყოს რიცხვები;\n",
    " * Tag-ები არ შეიძლება იყოს ერთ ან ორ სიმბოლოიანი სიტყვები;\n",
    " * სიტყვებად ჩავთვალოთ შემდეგი სიმბოლოებით დაყოფილი ტოკენები: \n",
    "\n",
    "          whitespace\n",
    "\n",
    "          ,\n",
    "\n",
    "          :\n",
    "\n",
    "          !\n",
    "\n",
    "          ?\n",
    "\n",
    "\n",
    "საბოლოო resultset უნდა ჩაწეროთ csv ფაილად და უნდა შედგებოდეს ველებისგან: ***ArticleID, Category, Tag, Frequency***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwDCgAC8ILSa",
    "outputId": "dfeebdfd-73d5-4c08-8853-18cecbca5971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+\n",
      "|ArticleID|Category|     Word|\n",
      "+---------+--------+---------+\n",
      "|     1833|business| worldcom|\n",
      "|     1833|business|  ex-boss|\n",
      "|     1833|business| launches|\n",
      "|     1833|business|  defence|\n",
      "|     1833|business|  lawyers|\n",
      "|     1833|business|defending|\n",
      "|     1833|business|   former|\n",
      "|     1833|business| worldcom|\n",
      "|     1833|business|    chief|\n",
      "|     1833|business|   bernie|\n",
      "+---------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, lit, array_except, length, size, array, explode, count, array_remove\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([ \\\n",
    "        StructField(\"ArticleID\",IntegerType(),True), \\\n",
    "        StructField(\"Text\",StringType(),True), \\\n",
    "        StructField(\"Category\",StringType(),True)\n",
    "      ])\n",
    "\n",
    "news = spark.read.option('inferSchema', 'false').schema(schema).csv('bbc_news.csv')\n",
    "stopwords = spark.read.option('header', 'true').csv('stopwords.txt')\n",
    "\n",
    "''' array_except დუბლირებებს შლის, რაც არ გვაწყობს '''\n",
    "# filtered = news\\\n",
    "#             .withColumn('Words', split(col('Text'), ' |\\.|,|\\:|\\!|\\?'))\\\n",
    "#             .withColumn('OriginalNumOfWords', lit(size(col('Words'))))\\\n",
    "#             .withColumn('WordsFiltered', array_except(col('Words'), array(*map(lit, stopwords))))\\\n",
    "#             .withColumn('ExceptStopwords', lit(size(col('WordsFiltered'))))\\\n",
    "\n",
    "# filtered.show(1, 0, True)\n",
    "\n",
    "exploded = news\\\n",
    "            .withColumn('Words', split(col('Text'), ' |\\.|,|\\:|\\!|\\?'))\\\n",
    "            .select(news.ArticleID, news.Category, explode(col('Words')).alias('Word'))\\\n",
    "            .filter(length(col('Word')) > 2)\\\n",
    "            .where(\"Word != '' \")\\\n",
    "\n",
    "drop_nums = exploded\\\n",
    "            .withColumn('IsNumber', col('Word').cast('int').isNotNull())\\\n",
    "            .where(~col('IsNumber'))\\\n",
    "            .drop('IsNumber')\\\n",
    "\n",
    "filter_stop_words = drop_nums.alias('w')\\\n",
    "                      .join(stopwords.alias('s'), col('Word') == col('i'), 'leftanti')\n",
    "\n",
    "filter_stop_words.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QW_NWTYYIXlO",
    "outputId": "bda9b40b-7a45-4b23-ebd0-4e3df0827631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+----+\n",
      "|ArticleID|     Category|    Word|Freq|\n",
      "+---------+-------------+--------+----+\n",
      "|      760|     business|    fiat|  16|\n",
      "|     1183|entertainment|   music|  13|\n",
      "|      484|     business|  glazer|  12|\n",
      "|     1229|         tech|  search|  14|\n",
      "|       20|         tech|argonaut|  12|\n",
      "|      435|     business|    card|  12|\n",
      "|      623|     politics|   trust|  14|\n",
      "|     1149|     politics|   local|  11|\n",
      "|     1928|        sport|   point|  18|\n",
      "|     2130|     business|  dollar|  11|\n",
      "|      701|     politics|  police|  13|\n",
      "|     1149|     politics|    said|  13|\n",
      "|     1976|         tech|   phone|  11|\n",
      "|      840|         tech|   firms|  11|\n",
      "|     2115|entertainment| academy|  12|\n",
      "|     1597|entertainment|   chart|  16|\n",
      "|      156|entertainment|     lee|  15|\n",
      "|      860|         tech|  mobile|  12|\n",
      "|     1624|         tech|bloggers|  11|\n",
      "|     1455|         tech|   yahoo|  24|\n",
      "+---------+-------------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frequencies = filter_stop_words\\\n",
    "              .groupby('ArticleID', 'Category', 'Word')\\\n",
    "              .agg(count('*').alias('Freq'))\\\n",
    "              .where('Freq > 10')\n",
    "\n",
    "frequencies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_Pcr-luIxRY"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "results = frequencies\\\n",
    "            .withColumn('rank', row_number()\\\n",
    "                        .over(\n",
    "                                Window.partitionBy('ArticleID', 'Category').orderBy(col('Freq').desc())\n",
    "                              ))\\\n",
    "            .where('rank <= 3')\\\n",
    "            .withColumnRenamed('Freq', 'Frequency')\\\n",
    "            .withColumnRenamed('Word', 'Tag')\\\n",
    "            .select('ArticleID', 'Category', 'Tag', 'Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn92_fpHOell"
   },
   "source": [
    "Save to various file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OvLhBu4JqkG"
   },
   "outputs": [],
   "source": [
    "# save to CSV\n",
    "results.write.mode('overwrite').csv('frequencies_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrTLLM7rKI1N"
   },
   "outputs": [],
   "source": [
    "''' save to Parquet '''\n",
    "\n",
    "results.write.mode('overwrite').parquet('frequencies_parquet.parquet')\n",
    "\n",
    "# spark.read.parquet('frequencies_parquet.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVOss7YrKUss",
    "outputId": "a4154efe-b474-4499-8b97-bffcaa93f2a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------+--------+\n",
      "|ArticleID|            Tag|Frequency|Category|\n",
      "+---------+---------------+---------+--------+\n",
      "|     1022|           said|       17|    tech|\n",
      "|     1022|          firms|       11|    tech|\n",
      "|     1022|      consumers|       11|    tech|\n",
      "|     1039|          apple|       12|    tech|\n",
      "|     1097|         online|       15|    tech|\n",
      "|      110|        hip-hop|       14|    tech|\n",
      "|     1111|high-definition|       13|    tech|\n",
      "|     1111|            dvd|       11|    tech|\n",
      "|     1114|           maps|       12|    tech|\n",
      "|     1114|        respond|       11|    tech|\n",
      "|     1135|            net|       16|    tech|\n",
      "|     1190|          phone|       15|    tech|\n",
      "|     1190|      companies|       12|    tech|\n",
      "|     1190|        premium|       11|    tech|\n",
      "|     1204|      commodore|       12|    tech|\n",
      "|     1215|          virus|       12|    tech|\n",
      "|     1229|         search|       14|    tech|\n",
      "|     1336|      musicians|       13|    tech|\n",
      "|     1363|     multimedia|       12|    tech|\n",
      "|     1395|           spam|       13|    tech|\n",
      "+---------+---------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Save to ORC and partition by Category ''' \n",
    "\n",
    "results \\\n",
    "  .write \\\n",
    "  .partitionBy('Category') \\\n",
    "  .option('orc.compress', 'snappy') \\\n",
    "  .mode('append') \\\n",
    "  .orc(\"frequencies_orc\")\n",
    "\n",
    "spark.read.orc('frequencies_orc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvo9qC_pOlAp"
   },
   "source": [
    "Spark SQL & Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HQ6OCtNLO0p",
    "outputId": "9318e405-a867-41d3-80b7-82d495032b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------+---------+\n",
      "|ArticleID|     Category|       Tag|Frequency|\n",
      "+---------+-------------+----------+---------+\n",
      "|       20|         tech|  argonaut|       12|\n",
      "|       25|         tech|technology|       12|\n",
      "|       25|         tech|     sound|       11|\n",
      "|       25|         tech|    mobile|       11|\n",
      "|       32|         tech|   firefox|       13|\n",
      "|       46|entertainment|     music|       26|\n",
      "|       46|entertainment|government|       22|\n",
      "|       68|entertainment|       bbc|       12|\n",
      "|       78|        sport|     break|       13|\n",
      "|       85|entertainment|      film|       11|\n",
      "|       93|     politics|    answer|       11|\n",
      "|       93|     politics|      said|       11|\n",
      "|      110|         tech|   hip-hop|       14|\n",
      "|      119|        sport|     james|       12|\n",
      "|      143|         tech|      said|       25|\n",
      "|      143|         tech|       mac|       19|\n",
      "|      143|         tech|      mini|       17|\n",
      "|      148|entertainment|      best|       11|\n",
      "|      155|         tech|    google|       13|\n",
      "|      156|entertainment|       lee|       15|\n",
      "+---------+-------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.createOrReplaceTempView(\"tags_tmp\")\n",
    "\n",
    "spark.table('tags_tmp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zON41QBsN9ht",
    "outputId": "ac5ba144-2b69-4120-ee61-1178a989dd8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|bbc      |\n",
      "|default  |\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |tags_tmp |true       |\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+---------+---------+-------+\n",
      "|col_name |data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|ArticleID|int      |null   |\n",
      "|Category |string   |null   |\n",
      "|Tag      |string   |null   |\n",
      "|Frequency|bigint   |null   |\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show(10, truncate=False)\n",
    "spark.sql('show tables in default').show(10, truncate=False)\n",
    "spark.sql('describe table extended tags_tmp').show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "SLXXLbZKOJ6D",
    "outputId": "82949e11-9489-4db2-b359-59e94db16eb9"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-ecd6729d1881>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'create schema bbc'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bbc.results_as_table'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'parquet'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitionBy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Category'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Tag'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery)\u001B[0m\n\u001B[1;32m    721\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'row1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'row2'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'row3'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    722\u001B[0m         \"\"\"\n\u001B[0;32m--> 723\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    724\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    725\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1308\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1309\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1310\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1312\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Database bbc already exists"
     ]
    }
   ],
   "source": [
    "spark.sql('create schema bbc');\n",
    "\n",
    "results.write.saveAsTable('bbc.results_as_table', format='parquet', partitionBy=['Category', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aetAadUPOMK",
    "outputId": "7341ed58-bb63-4bcb-8dc5-a239425bc9e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|bbc      |\n",
      "|default  |\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |tags_tmp |true       |\n",
      "+---------+---------+-----------+\n",
      "\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|ArticleID                   |string                                                        |null   |\n",
      "|Frequency                   |bigint                                                        |null   |\n",
      "|Category                    |string                                                        |null   |\n",
      "|Tag                         |string                                                        |null   |\n",
      "|# Partition Information     |                                                              |       |\n",
      "|# col_name                  |data_type                                                     |comment|\n",
      "|Category                    |string                                                        |null   |\n",
      "|Tag                         |string                                                        |null   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Database                    |bbc                                                           |       |\n",
      "|Table                       |results_as_table                                              |       |\n",
      "|Owner                       |root                                                          |       |\n",
      "|Created Time                |Mon Nov 22 20:01:34 UTC 2021                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 3.2.0                                                   |       |\n",
      "|Type                        |MANAGED                                                       |       |\n",
      "|Provider                    |parquet                                                       |       |\n",
      "|Location                    |file:/content/spark-warehouse/bbc.db/results_as_table         |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "|Partition Provider          |Catalog                                                       |       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n",
      "+---------+---------+-------------+----+\n",
      "|ArticleID|Frequency|     Category| Tag|\n",
      "+---------+---------+-------------+----+\n",
      "|     1149|       13|     politics|said|\n",
      "|     1170|       11|     politics|said|\n",
      "|     1368|       11|     politics|said|\n",
      "|     1498|       12|     politics|said|\n",
      "|     1534|       11|     politics|said|\n",
      "|      169|       11|     politics|said|\n",
      "|     1783|       12|     politics|said|\n",
      "|     1792|       13|     politics|said|\n",
      "|     1852|       12|     politics|said|\n",
      "|     1938|       11|     politics|said|\n",
      "|     1978|       13|     politics|said|\n",
      "|     2081|       14|     politics|said|\n",
      "|     2170|       14|     politics|said|\n",
      "|      749|       12|     politics|said|\n",
      "|      755|       16|     politics|said|\n",
      "|       93|       11|     politics|said|\n",
      "|      947|       12|     politics|said|\n",
      "|      951|       13|     politics|said|\n",
      "|     1149|       13|     politics|said|\n",
      "|     1170|       11|     politics|said|\n",
      "|     1368|       11|     politics|said|\n",
      "|     1498|       12|     politics|said|\n",
      "|     1534|       11|     politics|said|\n",
      "|      169|       11|     politics|said|\n",
      "|     1783|       12|     politics|said|\n",
      "|     1792|       13|     politics|said|\n",
      "|     1852|       12|     politics|said|\n",
      "|     1938|       11|     politics|said|\n",
      "|     1978|       13|     politics|said|\n",
      "|     2081|       14|     politics|said|\n",
      "|     2170|       14|     politics|said|\n",
      "|      749|       12|     politics|said|\n",
      "|      755|       16|     politics|said|\n",
      "|       93|       11|     politics|said|\n",
      "|      947|       12|     politics|said|\n",
      "|      951|       13|     politics|said|\n",
      "|     1149|       13|     politics|said|\n",
      "|     1170|       11|     politics|said|\n",
      "|     1368|       11|     politics|said|\n",
      "|     1498|       12|     politics|said|\n",
      "|     1534|       11|     politics|said|\n",
      "|      169|       11|     politics|said|\n",
      "|     1783|       12|     politics|said|\n",
      "|     1792|       13|     politics|said|\n",
      "|     1852|       12|     politics|said|\n",
      "|     1938|       11|     politics|said|\n",
      "|     1978|       13|     politics|said|\n",
      "|     2081|       14|     politics|said|\n",
      "|     2170|       14|     politics|said|\n",
      "|      749|       12|     politics|said|\n",
      "|      755|       16|     politics|said|\n",
      "|       93|       11|     politics|said|\n",
      "|      947|       12|     politics|said|\n",
      "|      951|       13|     politics|said|\n",
      "|     1022|       17|         tech|said|\n",
      "|      143|       25|         tech|said|\n",
      "|     1667|       11|         tech|said|\n",
      "|     1831|       14|         tech|said|\n",
      "|     1880|       13|         tech|said|\n",
      "|     1976|       12|         tech|said|\n",
      "|     2117|       12|         tech|said|\n",
      "|     2133|       13|         tech|said|\n",
      "|     2175|       12|         tech|said|\n",
      "|      240|       15|         tech|said|\n",
      "|      645|       17|         tech|said|\n",
      "|      840|       17|         tech|said|\n",
      "|     1022|       17|         tech|said|\n",
      "|      143|       25|         tech|said|\n",
      "|     1667|       11|         tech|said|\n",
      "|     1831|       14|         tech|said|\n",
      "|     1880|       13|         tech|said|\n",
      "|     1976|       12|         tech|said|\n",
      "|     2117|       12|         tech|said|\n",
      "|     2133|       13|         tech|said|\n",
      "|     2175|       12|         tech|said|\n",
      "|      240|       15|         tech|said|\n",
      "|      645|       17|         tech|said|\n",
      "|      840|       17|         tech|said|\n",
      "|     1022|       17|         tech|said|\n",
      "|      143|       25|         tech|said|\n",
      "|     1667|       11|         tech|said|\n",
      "|     1831|       14|         tech|said|\n",
      "|     1880|       13|         tech|said|\n",
      "|     1976|       12|         tech|said|\n",
      "|     2117|       12|         tech|said|\n",
      "|     2133|       13|         tech|said|\n",
      "|     2175|       12|         tech|said|\n",
      "|      240|       15|         tech|said|\n",
      "|      645|       17|         tech|said|\n",
      "|      840|       17|         tech|said|\n",
      "|     1081|       17|entertainment|best|\n",
      "|     1264|       26|entertainment|best|\n",
      "|      148|       11|entertainment|best|\n",
      "|     1500|       22|entertainment|best|\n",
      "|     1645|       11|entertainment|best|\n",
      "|     1875|       18|entertainment|best|\n",
      "|      190|       18|entertainment|best|\n",
      "|      274|       12|entertainment|best|\n",
      "|      366|       11|entertainment|best|\n",
      "|      902|       17|entertainment|best|\n",
      "+---------+---------+-------------+----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show(10, truncate=False)\n",
    "spark.sql('show tables in default').show(10, truncate=False)\n",
    "spark.sql('describe table extended bbc.results_as_table').show(100, truncate=False)\n",
    "\n",
    "spark.sql('select * from bbc.results_as_table').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqghWvSvQhu1",
    "outputId": "4b94c4bb-b375-4389-9148-b379d5281784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140\n",
      "1520\n"
     ]
    }
   ],
   "source": [
    "print(spark.table('bbc.results_as_table').count())\n",
    "\n",
    "cols = spark.table('bbc.results_as_table').columns \n",
    "\n",
    "results.select(*cols).write.insertInto('bbc.results_as_table', overwrite=False)\n",
    "spark.sql('MSCK repair table bbc.results_as_table')\n",
    "\n",
    "print(spark.table('bbc.results_as_table').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "f02IEKx2PhM5",
    "outputId": "5ecabb42-cf0d-4335-cc94-202de624a0bb"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-8acfaccd81d2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mresults\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'path'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'bbc/results'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bbc.results_as_external_table'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'parquet'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'describe table extended bbc.results_as_external_table'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36msaveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    804\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mformat\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    805\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 806\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    807\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    808\u001B[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1308\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1309\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1310\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1312\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Table `bbc`.`results_as_external_table` already exists."
     ]
    }
   ],
   "source": [
    "results.write.option('path', 'bbc/results').saveAsTable('bbc.results_as_external_table', format='parquet')\n",
    "\n",
    "spark.sql('describe table extended bbc.results_as_external_table').show(100, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PySpark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}